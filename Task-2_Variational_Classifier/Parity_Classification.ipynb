{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we will implement two quantum variational classifiers. First, we will train a quantum variational circuit to emulate the parity function \n",
    "$$\n",
    "f : x \\in \\{0,1\\}^{\\otimes n} \\rightarrow y = \\begin{cases} \n",
    "1 & \\text{if odd number of 1's in } x \\\\\n",
    "0 & \\text{else}.\n",
    "\\end{cases}\n",
    "$$\n",
    "And second, we will train another quantum variational circuit to recognize the first two classes of flowers in the iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parity Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the basic workflow in Quantum Machine Learning: \n",
    "\n",
    "1. **Encoding**: we encode the model's input into the initial state of the quantum circuit. In the present example, basis encoding lends itself most naturally, since the model's input is a bit string.\n",
    "\n",
    "2. **Evolving**: we evolve the input state by applying a series of parameterized unitary operators, with the parameters acting as the weights of our quantum neural network.\n",
    "\n",
    "3. **Measurement**: we perform a specific measurement on the final state of our circuit. The resulting value will be our model's prediction of the function's output for the specified encoded input in step 1.\n",
    "\n",
    "4. **Optimizing**: we use an optimizer to update our parameters used in step 2 using a gradient-based method. In our present case, we use `NesterovMomentumOptimizer`, which implements the Nesterov Momentum optimization technique that improves the convergence speed of gradient descent algorithms by incorporating momentum.\n",
    "\n",
    "In step 2, we evolve the state by applying *layers*, each composed of an arbitrary rotation (most general unitary with 3 free parameters) on each individual qubit, along with a ring of CNOT gates. Our full variational circuit for two layers should look like this:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img/parity_function_image.png\" alt=\"Alt Text\" style=\"width:900px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the block \"$\\ket\\psi$\" denotes our basis encoding routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now begin implementing the model by importing the necessary packages and creating our quantum device to implement the circuit. We choose the default pennylane simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\") #default pennylane simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a `state_prepararion` function that simply does the basis encoding for us. We define a `layer` function that takes the $4 \\times 3$ array corresponding to the layer's weights (3 wights per each of the 4 qubits) and implements the corresponding rotation as well as the ring of CNOTS. \n",
    "\n",
    "We next define our variational circuit that performs all of the above + performs a measurment on our system. And finally we define the `variatoinal_classifier` function that adds to our output a bias. This will be the model's final output to be referenced later by our `cost` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_preparation(x):\n",
    "    qml.BasisState(x, wires=[0, 1, 2, 3])\n",
    "\n",
    "def layer(layer_weights):\n",
    "    for wire in range(4):\n",
    "        #unpacking the layer's weights and passing them as angles to qml.Rot()\n",
    "        qml.Rot(*layer_weights[wire], wires=wire) \n",
    "\n",
    "    for wires in ([0, 1], [1, 2], [2, 3], [3, 0]):\n",
    "        qml.CNOT(wires)\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(weights, x):\n",
    "    state_preparation(x) #Step 1: (Basis) Encoding\n",
    "\n",
    "    for layer_weights in weights:\n",
    "        layer(layer_weights) #Step 2: Evolving\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0)) #Step 3: Measurement\n",
    "\n",
    "\n",
    "#incorporating a bias into the output\n",
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our optimizer, ofcourse, neeeds a cost function to minimize. We take the standard mean square difference as our loss function, which gives the \"distance\" between each input's label and our model's output.\n",
    "\n",
    "We also define an \"accuracy\" fucntion which gives a measure of the accuracy of the model's output, for monitoring purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    # We use a call to qml.math.stack to allow subtracting the arrays directly\n",
    "    return np.mean((labels - qml.math.stack(predictions)) ** 2)\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n",
    "    acc = acc / len(labels)\n",
    "    return acc\n",
    "\n",
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load in the training data into $\\verb|X|$, the corresponding labels into $\\verb|Y|$, and shift the labels from {0, 1} to {-1, 1}, which will make it wasier for our model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0 0 0 1], y = 1\n",
      "x = [0 0 1 0], y = 1\n",
      "x = [0 1 0 0], y = 1\n",
      "x = [0 1 0 1], y = -1\n",
      "x = [0 1 1 0], y = -1\n",
      "x = [0 1 1 1], y = 1\n",
      "x = [1 0 0 0], y = 1\n",
      "x = [1 0 0 1], y = -1\n",
      "x = [1 0 1 1], y = 1\n",
      "x = [1 1 1 1], y = -1\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/parity_train.txt\", dtype=int)\n",
    "X = np.array(data[:, :-1])\n",
    "Y = np.array(data[:, -1])\n",
    "Y = Y * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "for x,y in zip(X, Y):\n",
    "    print(f\"x = {x}, y = {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialize our model. We need 4 qubits since the model's input is a 4-bit string. We specify the number of layers to 2. And we initialize the weights pseudorandomly (with a specified seed for reproducability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: \n",
      " [[[ 0.01764052  0.00400157  0.00978738]\n",
      "  [ 0.02240893  0.01867558 -0.00977278]\n",
      "  [ 0.00950088 -0.00151357 -0.00103219]\n",
      "  [ 0.00410599  0.00144044  0.01454274]]\n",
      "\n",
      " [[ 0.00761038  0.00121675  0.00443863]\n",
      "  [ 0.00333674  0.01494079 -0.00205158]\n",
      "  [ 0.00313068 -0.00854096 -0.0255299 ]\n",
      "  [ 0.00653619  0.00864436 -0.00742165]]]\n",
      "Bias:  0.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "print(\"Weights: \\n\", weights_init)\n",
    "print(\"Bias: \", bias_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize our optimizer with a learning rate of 0.5, and specify the learning batch size to 5 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = NesterovMomentumOptimizer(0.5)\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now begin the learning process. We iterate over the following steps:\n",
    "1. we randomly sample 5 inputs from our input array $\\verb|X|$, along with their labels.\n",
    "2. we pass the sampled data along with the current iteration's weights to the optimizer\n",
    "3. the optimizer updates the weights (and bias) to new \"better\" values such that the model better approximates $f(x)$\n",
    "4. given the new weights, we run the model over all of our inputs and get the predictions for the current iteration\n",
    "5. we print out the current cost and accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:    1 | Cost: 2.3147651 | Accuracy: 0.5000000\n",
      "Iter:    2 | Cost: 1.9664866 | Accuracy: 0.5000000\n",
      "Iter:    3 | Cost: 1.9208589 | Accuracy: 0.5000000\n",
      "Iter:    4 | Cost: 2.6276126 | Accuracy: 0.5000000\n",
      "Iter:    5 | Cost: 0.9323119 | Accuracy: 0.6000000\n",
      "Iter:    6 | Cost: 1.1903549 | Accuracy: 0.5000000\n",
      "Iter:    7 | Cost: 2.0508989 | Accuracy: 0.4000000\n",
      "Iter:    8 | Cost: 1.1275531 | Accuracy: 0.6000000\n",
      "Iter:    9 | Cost: 1.1659803 | Accuracy: 0.6000000\n",
      "Iter:   10 | Cost: 1.1349618 | Accuracy: 0.6000000\n",
      "Iter:   11 | Cost: 0.9994063 | Accuracy: 0.6000000\n",
      "Iter:   12 | Cost: 1.0812559 | Accuracy: 0.6000000\n",
      "Iter:   13 | Cost: 1.2863155 | Accuracy: 0.6000000\n",
      "Iter:   14 | Cost: 2.2658259 | Accuracy: 0.4000000\n",
      "Iter:   15 | Cost: 1.1323724 | Accuracy: 0.6000000\n",
      "Iter:   16 | Cost: 1.3439737 | Accuracy: 0.8000000\n",
      "Iter:   17 | Cost: 2.0076168 | Accuracy: 0.6000000\n",
      "Iter:   18 | Cost: 1.2685760 | Accuracy: 0.5000000\n",
      "Iter:   19 | Cost: 1.6762475 | Accuracy: 0.5000000\n",
      "Iter:   20 | Cost: 1.1868237 | Accuracy: 0.6000000\n",
      "Iter:   21 | Cost: 1.4784687 | Accuracy: 0.6000000\n",
      "Iter:   22 | Cost: 1.4599473 | Accuracy: 0.6000000\n",
      "Iter:   23 | Cost: 0.9573269 | Accuracy: 0.6000000\n",
      "Iter:   24 | Cost: 1.1657424 | Accuracy: 0.5000000\n",
      "Iter:   25 | Cost: 1.0877087 | Accuracy: 0.4000000\n",
      "Iter:   26 | Cost: 1.1683687 | Accuracy: 0.6000000\n",
      "Iter:   27 | Cost: 2.1141689 | Accuracy: 0.6000000\n",
      "Iter:   28 | Cost: 1.0272966 | Accuracy: 0.5000000\n",
      "Iter:   29 | Cost: 0.9664085 | Accuracy: 0.5000000\n",
      "Iter:   30 | Cost: 1.1287654 | Accuracy: 0.6000000\n",
      "Iter:   31 | Cost: 1.4202360 | Accuracy: 0.4000000\n",
      "Iter:   32 | Cost: 1.1286000 | Accuracy: 0.5000000\n",
      "Iter:   33 | Cost: 1.9594333 | Accuracy: 0.4000000\n",
      "Iter:   34 | Cost: 1.2811832 | Accuracy: 0.4000000\n",
      "Iter:   35 | Cost: 0.8522775 | Accuracy: 0.7000000\n",
      "Iter:   36 | Cost: 1.4765281 | Accuracy: 0.6000000\n",
      "Iter:   37 | Cost: 0.9603287 | Accuracy: 0.6000000\n",
      "Iter:   38 | Cost: 1.6031314 | Accuracy: 0.6000000\n",
      "Iter:   39 | Cost: 1.1700888 | Accuracy: 0.4000000\n",
      "Iter:   40 | Cost: 1.7571779 | Accuracy: 0.4000000\n",
      "Iter:   41 | Cost: 1.9608116 | Accuracy: 0.6000000\n",
      "Iter:   42 | Cost: 2.0802752 | Accuracy: 0.6000000\n",
      "Iter:   43 | Cost: 1.1904884 | Accuracy: 0.3000000\n",
      "Iter:   44 | Cost: 0.9941585 | Accuracy: 0.6000000\n",
      "Iter:   45 | Cost: 1.0709609 | Accuracy: 0.5000000\n",
      "Iter:   46 | Cost: 0.9780625 | Accuracy: 0.6000000\n",
      "Iter:   47 | Cost: 1.1573709 | Accuracy: 0.6000000\n",
      "Iter:   48 | Cost: 1.0235239 | Accuracy: 0.6000000\n",
      "Iter:   49 | Cost: 1.2842469 | Accuracy: 0.5000000\n",
      "Iter:   50 | Cost: 0.8549226 | Accuracy: 0.6000000\n",
      "Iter:   51 | Cost: 0.5136787 | Accuracy: 1.0000000\n",
      "Iter:   52 | Cost: 0.2488031 | Accuracy: 1.0000000\n",
      "Iter:   53 | Cost: 0.0461277 | Accuracy: 1.0000000\n",
      "Iter:   54 | Cost: 0.0293518 | Accuracy: 1.0000000\n",
      "Iter:   55 | Cost: 0.0205454 | Accuracy: 1.0000000\n",
      "Iter:   56 | Cost: 0.0352514 | Accuracy: 1.0000000\n",
      "Iter:   57 | Cost: 0.0576767 | Accuracy: 1.0000000\n",
      "Iter:   58 | Cost: 0.0291305 | Accuracy: 1.0000000\n",
      "Iter:   59 | Cost: 0.0127137 | Accuracy: 1.0000000\n",
      "Iter:   60 | Cost: 0.0058108 | Accuracy: 1.0000000\n"
     ]
    }
   ],
   "source": [
    "weights = weights_init\n",
    "bias = bias_init\n",
    "\n",
    "#Step 4: Optimizing\n",
    "for it in range(60):\n",
    "\n",
    "    # Update the weights by one optimizer step, using only a limited batch of data\n",
    "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "    weights, bias = opt.step(cost, weights, bias, X=X_batch, Y=Y_batch)\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "\n",
    "    current_cost = cost(weights, bias, X, Y)\n",
    "    acc = accuracy(Y, predictions)\n",
    "\n",
    "    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the learning process is completed, the model now computes $f(x)$ to a very high accuracy for all of the training data. We now wish to test how well it generalizes by feeding it new \"unseen\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0 0 0 0], y = -1, prediction = -1.0\n",
      "x = [0 0 1 1], y = -1, prediction = -1.0\n",
      "x = [1 0 1 0], y = -1, prediction = -1.0\n",
      "x = [1 1 1 0], y = 1, prediction = 1.0\n",
      "x = [1 1 0 0], y = -1, prediction = -1.0\n",
      "x = [1 1 0 1], y = 1, prediction = 1.0\n",
      "Accuracy on unseen data: 1.0\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/parity_test.txt\", dtype=int)\n",
    "X_test = np.array(data[:, :-1])\n",
    "Y_test = np.array(data[:, -1])\n",
    "Y_test = Y_test * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "predictions_test = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
    "\n",
    "for x,y,p in zip(X_test, Y_test, predictions_test):\n",
    "    print(f\"x = {x}, y = {y}, prediction = {p}\")\n",
    "\n",
    "acc_test = accuracy(Y_test, predictions_test)\n",
    "print(\"Accuracy on unseen data:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our job is now done since the model continues to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our next example in \"Iris_Classification.ipynb\" shows an extention of the quantum variational classifier model to a deal with a problem with a slightly more complex data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Classifier [Pennylane Demo](https://pennylane.ai/qml/demos/tutorial_variational_classifier/) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML_Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
